{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "from itertools import permutations, combinations, cycle\n",
    "import os \n",
    "from random import sample, shuffle \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import cv2 as cv\n",
    "import sklearn.metrics\n",
    "import re\n",
    "from efficientnet import tfkeras as efn \n",
    "from pathlib import Path\n",
    "import rasterio \n",
    "\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, EarlyStopping, CSVLogger, TensorBoard, ReduceLROnPlateau\n",
    "import segmentation_models as sm\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import wkt\n",
    "\n",
    "from typing import List, Tuple\n",
    "from slacker import Slacker\n",
    "slack = Slacker('xoxp-406617419703-407736556887-975525827328-1c7c24b94d95408268b84ada0b16d937')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_COMMON_PATH = Path('train/AOI_11_Rotterdam')\n",
    "TRAIN_SAR_PATH = TRAIN_COMMON_PATH/'SAR-Intensity'\n",
    "TRAIN_GT_PATH = TRAIN_COMMON_PATH/'train_ground_truth'\n",
    "TEST_SAR_PATH = None #TODO\n",
    "TEST_GT_PATH = None #TODO\n",
    "\n",
    "FILENAME_PATTERN = re.compile('SN6_Train_AOI_11_Rotterdam_SAR-Intensity_(\\d*_\\d*_tile_\\d*).tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_to_display(x, n_channels, normalize=True):\n",
    "    preprocessed = x.copy()\n",
    "    if normalize:\n",
    "        preprocessed = preprocessed/preprocessed.max()\n",
    "    if n_channels > 1:\n",
    "        preprocessed = np.moveaxis(preprocessed[:3], 0, -1)\n",
    "    else: \n",
    "        preprocessed = preprocessed[0]\n",
    "    return preprocessed\n",
    "\n",
    "def get_sar_imagery_statistics(path):\n",
    "    array = get_array_from_tiff(path)\n",
    "    means = array.mean(axis=(1,2))\n",
    "    stds = array.std(axis=(1,2))\n",
    "    return means, stds\n",
    "\n",
    "def get_array_from_tiff(path):\n",
    "    with rasterio.open(path) as file: \n",
    "        im = file.read()\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>TileBuildingId</th>\n",
       "      <th>PolygonWKT_Pix</th>\n",
       "      <th>Mean_Building_Height</th>\n",
       "      <th>Median_Building_Height</th>\n",
       "      <th>StdDev_Building_Height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190822070610_20190822070846_tile_3721</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((299.6396801332012 349.3765436094254,...</td>\n",
       "      <td>9.962397</td>\n",
       "      <td>9.96</td>\n",
       "      <td>0.006495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190822070610_20190822070846_tile_3721</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((115.5360228798818 339.845588516444, ...</td>\n",
       "      <td>2.810000</td>\n",
       "      <td>2.81</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190822070610_20190822070846_tile_3721</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((768.9086768317502 329.8960437048227,...</td>\n",
       "      <td>14.420000</td>\n",
       "      <td>14.42</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190822070610_20190822070846_tile_3721</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((755.8174585120287 330.0953964963555,...</td>\n",
       "      <td>14.420000</td>\n",
       "      <td>14.42</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190822070610_20190822070846_tile_3721</td>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((392.8786215754226 335.6222213506699,...</td>\n",
       "      <td>8.590000</td>\n",
       "      <td>8.59</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214677</th>\n",
       "      <td>20190822133333_20190822133635_tile_7758</td>\n",
       "      <td>5</td>\n",
       "      <td>POLYGON ((241.8162563492078 165.4673625379801,...</td>\n",
       "      <td>5.970000</td>\n",
       "      <td>5.97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214678</th>\n",
       "      <td>20190822133333_20190822133635_tile_7758</td>\n",
       "      <td>6</td>\n",
       "      <td>POLYGON ((431.9838383866008 146.2224273793399,...</td>\n",
       "      <td>10.380000</td>\n",
       "      <td>10.38</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214679</th>\n",
       "      <td>20190822133333_20190822133635_tile_7758</td>\n",
       "      <td>7</td>\n",
       "      <td>POLYGON ((128.6518265847117 111.3799640219659,...</td>\n",
       "      <td>3.870000</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214680</th>\n",
       "      <td>20190822133333_20190822133635_tile_7758</td>\n",
       "      <td>8</td>\n",
       "      <td>POLYGON ((415.3016002546065 98.69750475697219,...</td>\n",
       "      <td>11.710000</td>\n",
       "      <td>11.71</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214681</th>\n",
       "      <td>20190822133333_20190822133635_tile_7758</td>\n",
       "      <td>9</td>\n",
       "      <td>POLYGON ((835.701504310593 124.1703369021416, ...</td>\n",
       "      <td>6.410000</td>\n",
       "      <td>6.41</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214682 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ImageId  TileBuildingId  \\\n",
       "0       20190822070610_20190822070846_tile_3721               0   \n",
       "1       20190822070610_20190822070846_tile_3721               1   \n",
       "2       20190822070610_20190822070846_tile_3721               2   \n",
       "3       20190822070610_20190822070846_tile_3721               3   \n",
       "4       20190822070610_20190822070846_tile_3721               4   \n",
       "...                                         ...             ...   \n",
       "214677  20190822133333_20190822133635_tile_7758               5   \n",
       "214678  20190822133333_20190822133635_tile_7758               6   \n",
       "214679  20190822133333_20190822133635_tile_7758               7   \n",
       "214680  20190822133333_20190822133635_tile_7758               8   \n",
       "214681  20190822133333_20190822133635_tile_7758               9   \n",
       "\n",
       "                                           PolygonWKT_Pix  \\\n",
       "0       POLYGON ((299.6396801332012 349.3765436094254,...   \n",
       "1       POLYGON ((115.5360228798818 339.845588516444, ...   \n",
       "2       POLYGON ((768.9086768317502 329.8960437048227,...   \n",
       "3       POLYGON ((755.8174585120287 330.0953964963555,...   \n",
       "4       POLYGON ((392.8786215754226 335.6222213506699,...   \n",
       "...                                                   ...   \n",
       "214677  POLYGON ((241.8162563492078 165.4673625379801,...   \n",
       "214678  POLYGON ((431.9838383866008 146.2224273793399,...   \n",
       "214679  POLYGON ((128.6518265847117 111.3799640219659,...   \n",
       "214680  POLYGON ((415.3016002546065 98.69750475697219,...   \n",
       "214681  POLYGON ((835.701504310593 124.1703369021416, ...   \n",
       "\n",
       "        Mean_Building_Height  Median_Building_Height  StdDev_Building_Height  \n",
       "0                   9.962397                    9.96                0.006495  \n",
       "1                   2.810000                    2.81                0.000000  \n",
       "2                  14.420000                   14.42                0.000000  \n",
       "3                  14.420000                   14.42                0.000000  \n",
       "4                   8.590000                    8.59                0.000000  \n",
       "...                      ...                     ...                     ...  \n",
       "214677              5.970000                    5.97                0.000000  \n",
       "214678             10.380000                   10.38                0.000000  \n",
       "214679              3.870000                    3.87                0.000000  \n",
       "214680             11.710000                   11.71                0.000000  \n",
       "214681              6.410000                    6.41                0.000000  \n",
       "\n",
       "[214682 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildings = pd.read_csv(TRAIN_COMMON_PATH/'SummaryData/SN6_Train_AOI_11_Rotterdam_Buildings.csv',engine='python')\n",
    "buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_filename(filename):\n",
    "    return FILENAME_PATTERN.match(filename)[1]\n",
    "\n",
    "def get_polygons_in_image(rstr_filename):\n",
    "    image_id = get_id_from_filename(rstr_filename)\n",
    "    return buildings.loc[buildings['ImageId']==image_id,'PolygonWKT_Pix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3401"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.unique(buildings['ImageId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET_SIZE=200840\n",
    "#TRAINING_DATASET_SIZE =200\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "\n",
    "\n",
    "\n",
    "class ImageGen:\n",
    "    def __init__(self, mode=\"fit\", shuffle=False, batch_type='full_image', batch_size=32, train_val_frac = 0.8, verbose=False):\n",
    "        \n",
    "        assert not (mode=='test' and shuffle==True), 'Error: in test mode, the values should not be shuffled.'\n",
    "\n",
    "        self.batch_size = batch_size \n",
    "        if mode == \"fit\":\n",
    "            self.image_path = TRAIN_SAR_PATH\n",
    "            self.gt_path = TRAIN_GT_PATH\n",
    "        if mode == \"test\":\n",
    "            self.image_path = TEST_SAR_PATH\n",
    "            self.gt_path = TEST_GT_PATH\n",
    "        \n",
    "        self.generators = {}\n",
    "        self.orientations = pd.read_csv('train/AOI_11_Rotterdam/SummaryData/SAR_orientations.txt',sep=' ', header=None)\n",
    "        self.orientations.columns = [\"image_timestamps\",\"orientation\"]\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        image_files = os.listdir(self.image_path)\n",
    "        if shuffle == True: \n",
    "            shuffle(image_files)\n",
    "        if batch_type == 'full_image':\n",
    "            if mode == 'fit':\n",
    "                n_train = int(train_val_frac*len(image_files))\n",
    "                self.generators[\"train\"] = cycle((x for x in image_files[:n_train]))\n",
    "                self.generators['validation'] = cycle((x for x in image_files[n_train:]))\n",
    "            elif mode == \"test\":\n",
    "                raise ValueError(\"Test mode not implemented yet.\")\n",
    "                \n",
    "    def print_if_verbose(self, *args, status='always'):\n",
    "        if self.verbose and status=='always':\n",
    "            print(*args)\n",
    "        if self.verbose=='debug' and status=='debug':\n",
    "            print(*args)\n",
    "\n",
    "    def normalize(self,batch,normalization_type=None):\n",
    "        if normalization_type is None:\n",
    "            normalized_batch=batch \n",
    "        elif normalization_type=='divide':\n",
    "            normalized_batch=batch/255\n",
    "        return normalized_batch\n",
    "\n",
    "    def get_x_batch(self, image_ids_to_get):\n",
    "        batch = get_array_from_tiff(self.image_path/image_ids_to_get)\n",
    "        x_batch = np.ndarray(shape=(4,HEIGHT,WIDTH))\n",
    "        for i in range(4):\n",
    "            x_batch[i] = cv.resize(batch[i],dsize=(WIDTH,HEIGHT))\n",
    "        x_batch = self.normalize(x_batch)\n",
    "        x_batch = np.expand_dims(np.rollaxis(x_batch, 0, 3),axis=0)[...,:3]\n",
    "        return x_batch \n",
    "\n",
    "    def get_y_batch(self, image_ids_to_get):\n",
    "        batch = np.rollaxis(get_array_from_tiff(self.gt_path/image_ids_to_get)[0], 0, 2)\n",
    "        batch = np.rot90(batch, k=3)\n",
    "        batch = np.flip(batch,axis=1)\n",
    "        batch = cv.resize(batch,dsize=(WIDTH,HEIGHT))\n",
    "        batch = np.uint8(batch > 0)\n",
    "        batch = np.expand_dims(batch,axis=0)\n",
    "        batch = np.expand_dims(batch,axis=-1)\n",
    "        return batch \n",
    "    \n",
    "    def flow(self, mode: str =\"train\", height: int =137,width: int =236):\n",
    "        '''Run the generator '''\n",
    "        c = 0\n",
    "        while True:\n",
    "            image_ids_to_get = next(self.generators[mode])\n",
    "            image_orientation = self.orientations.loc[self.orientations[\"image_timestamps\"]==re.match('.*(\\d{14}_\\d{14})',image_ids_to_get)[1],\"orientation\"]\n",
    "            self.print_if_verbose(\"\\n INFO - image_ids_to_get:\", image_ids_to_get,\"\\n\",status='always')\n",
    "            self.print_if_verbose(\"\\n INFO - image_orientation:\",image_orientation)\n",
    "            x_batch = self.get_x_batch(image_ids_to_get)\n",
    "            if mode in ('train','validation'):\n",
    "                self.print_if_verbose(\"\\n INFO - current mode \",mode,status=\"debug\")\n",
    "                y_batch = self.get_y_batch(image_ids_to_get)\n",
    "                yield x_batch, y_batch\n",
    "            elif mode=='test':\n",
    "                c+=1\n",
    "                print(f\"\\n INFO - Yielding test data n°{c}/{self.batch_size/self.files_size['test']}\")\n",
    "                yield x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet()\n",
    "model.compile(\n",
    "    'Adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[sm.metrics.iou_score],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_generator = ImageGen(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FRAC = 0.8\n",
    "N_TRAIN = 3401\n",
    "STEP_PER_EPOCH = int(TRAIN_FRAC*N_TRAIN)\n",
    "#STEP_PER_EPOCH= 10\n",
    "VAL_STEPS_PER_EPOCH = int((1-TRAIN_FRAC)*N_TRAIN)\n",
    "#VAL_STEPS_PER_EPOCH= 10\n",
    "N_EPOCHS = 40\n",
    "LOG_DIR = 'logs/'\n",
    "MODELS_DIR = \"models/\"\n",
    "LOAD_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 23129864\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  9 08:52 model_weights-15-0.1981.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  9 08:02 model_weights-14-0.1975.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  9 07:12 model_weights-13-0.2059.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  9 06:22 model_weights-12-0.1965.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  9 05:30 model_weights-11-0.1909.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  9 04:41 model_weights-10-0.1619.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  9 03:49 model_weights-09-0.1983.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  9 03:00 model_weights-08-0.1816.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  9 02:10 model_weights-07-0.1694.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  9 01:21 model_weights-06-0.2058.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  9 00:32 model_weights-05-0.1994.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 23:42 model_weights-04-0.2044.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 22:53 model_weights-03-0.1866.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 22:03 model_weights-02-0.1939.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 21:14 model_weights-01-0.2091.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 20:07 model_weights-02-0.2221.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 19:43 model_weights-01-0.1455.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 18:42 model_weights-05-0.1714.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 17:36 model_weights-04-0.1749.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 16:39 model_weights-03-0.1743.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 15:48 model_weights-02-0.1771.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 14:57 model_weights-01-0.2154.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 11:27 model_weights-15-0.1746.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 11:09 model_weights-14-0.1672.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 10:50 model_weights-13-0.1644.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 10:32 model_weights-12-0.1729.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 10:14 model_weights-11-0.1721.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 09:55 model_weights-10-0.1504.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 09:37 model_weights-09-0.1687.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 09:19 model_weights-08-0.1812.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 08:57 model_weights-07-0.1527.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 08:30 model_weights-06-0.1801.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 08:09 model_weights-05-0.1644.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 07:52 model_weights-04-0.1896.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 07:31 model_weights-03-0.1901.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 07:12 model_weights-02-0.1638.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  8 06:55 model_weights-01-0.1892.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 22:38 model_weights-10-0.1722.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 22:21 model_weights-09-0.1678.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 21:58 model_weights-08-0.1809.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 21:40 model_weights-07-0.1828.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 21:22 model_weights-06-0.1903.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 21:05 model_weights-05-0.1455.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 20:48 model_weights-04-0.2340.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 20:29 model_weights-03-0.1921.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 20:11 model_weights-02-0.1918.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 19:54 model_weights-01-0.2022.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 18:43 model_weights-11-0.1680.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 18:25 model_weights-10-0.1958.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 18:06 model_weights-09-0.2100.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 17:49 model_weights-08-0.1855.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 17:32 model_weights-07-0.2041.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 17:14 model_weights-06-0.1620.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 16:57 model_weights-05-0.2107.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 16:39 model_weights-04-0.1897.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 16:22 model_weights-03-0.1860.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 16:03 model_weights-02-0.2045.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 15:46 model_weights-01-0.2014.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 14:49 model_weights-15-0.1716.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 14:30 model_weights-14-0.1781.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 14:13 model_weights-13-0.1609.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 13:56 model_weights-12-0.1957.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 13:38 model_weights-11-0.2083.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 13:21 model_weights-10-0.1607.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 13:03 model_weights-09-0.1802.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 12:44 model_weights-08-0.1698.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 12:27 model_weights-07-0.1655.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 12:09 model_weights-06-0.1721.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 11:51 model_weights-05-0.1722.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 11:33 model_weights-04-0.1691.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 11:15 model_weights-03-0.1786.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 10:58 model_weights-02-0.1778.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 10:40 model_weights-01-0.1855.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 09:54 model_weights-02-0.2303.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 09:05 model_weights-01-0.2241.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357208 Apr  7 08:00 model_weights-01-0.2019.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357576 Apr  7 07:04 model_weights-01-0.2072.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357576 Apr  7 06:42 model_weights-06-0.2284.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357576 Apr  7 06:24 model_weights-05-0.2306.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357576 Apr  7 06:06 model_weights-04-0.2098.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357576 Apr  7 05:48 model_weights-03-0.2011.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357576 Apr  7 05:30 model_weights-02-0.1980.hdf5\r\n",
      "-rw-r--r-- 1 root root 285357576 Apr  7 05:13 model_weights-01-0.1977.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lt models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, EarlyStopping, CSVLogger, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(MODELS_DIR+\"model_weights-{epoch:02d}-{val_loss:.4f}--{val_iou_score:.4f}.hdf5\", \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=False, \n",
    "                             mode='min')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=5,\n",
    "                              verbose=0, mode='auto')\n",
    "csv_logger = CSVLogger(LOG_DIR+'training.log')\n",
    "\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=3, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    model.load_weights(f'{MODELS_DIR}/model_weights-15-0.1981.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2720 steps, validate for 680 steps\n",
      "Epoch 1/40\n",
      "2719/2720 [============================>.] - ETA: 0s - loss: 0.0281 - iou_score: 0.7252\n",
      "Epoch 00001: saving model to models/model_weights-01-0.2771--0.2840.hdf5\n",
      "2720/2720 [==============================] - 1089s 401ms/step - loss: 0.0281 - iou_score: 0.7252 - val_loss: 0.2771 - val_iou_score: 0.2840\n",
      "Epoch 2/40\n",
      "2719/2720 [============================>.] - ETA: 0s - loss: 0.0278 - iou_score: 0.7270\n",
      "Epoch 00002: saving model to models/model_weights-02-0.2782--0.2860.hdf5\n",
      "2720/2720 [==============================] - 1065s 392ms/step - loss: 0.0279 - iou_score: 0.7270 - val_loss: 0.2782 - val_iou_score: 0.2860\n",
      "Epoch 3/40\n",
      "2719/2720 [============================>.] - ETA: 0s - loss: 0.0277 - iou_score: 0.7283\n",
      "Epoch 00003: saving model to models/model_weights-03-0.2880--0.2829.hdf5\n",
      "2720/2720 [==============================] - 1074s 395ms/step - loss: 0.0277 - iou_score: 0.7283 - val_loss: 0.2880 - val_iou_score: 0.2829\n",
      "Epoch 4/40\n",
      "2719/2720 [============================>.] - ETA: 0s - loss: 0.0275 - iou_score: 0.7298\n",
      "Epoch 00004: saving model to models/model_weights-04-0.2914--0.2852.hdf5\n",
      "2720/2720 [==============================] - 1085s 399ms/step - loss: 0.0275 - iou_score: 0.7297 - val_loss: 0.2914 - val_iou_score: 0.2852\n",
      "Epoch 5/40\n",
      "2719/2720 [============================>.] - ETA: 0s - loss: 0.0273 - iou_score: 0.7309\n",
      "Epoch 00005: saving model to models/model_weights-05-0.2993--0.2814.hdf5\n",
      "2720/2720 [==============================] - 1147s 422ms/step - loss: 0.0273 - iou_score: 0.7309 - val_loss: 0.2993 - val_iou_score: 0.2814\n",
      "Epoch 6/40\n",
      "2719/2720 [============================>.] - ETA: 0s - loss: 0.0272 - iou_score: 0.7323\n",
      "Epoch 00006: saving model to models/model_weights-06-0.2983--0.2848.hdf5\n",
      "2720/2720 [==============================] - 1252s 460ms/step - loss: 0.0272 - iou_score: 0.7322 - val_loss: 0.2983 - val_iou_score: 0.2848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbc36a0a0b8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit mo\n",
    "model.fit_generator(\n",
    "   fit_generator.flow(mode='train'),\n",
    "   epochs=N_EPOCHS,\n",
    "   steps_per_epoch=STEP_PER_EPOCH,\n",
    "   validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "   validation_data=fit_generator.flow(mode='validation'),\n",
    "   callbacks = [checkpoint, \n",
    "                early_stopping, \n",
    "                csv_logger, \n",
    "                reduce_lr_on_plateau\n",
    "                ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(x):\n",
    "    y = x.copy()\n",
    "    y[y==0]=1\n",
    "    return 10*np.log10(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = next(fit_generator.flow(mode='validation'))\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize=(15,9))\n",
    "pred = model.predict(img[0])\n",
    "ax = ax.ravel()\n",
    "for i in range(3):\n",
    "    ax[i].imshow(img[0][0,...,i],cmap='gist_gray',vmin=15,vmax=60)\n",
    "    ax[3+i].imshow(safe_log(img[0][0,...,i]),cmap='gist_gray',vmin=11,vmax=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(fit_generator.flow(mode='validation'))\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize=(20,20))\n",
    "pred = model.predict(img[0])\n",
    "for i in range(3):\n",
    "    ax[0,i].imshow(img[0][0,...,i],cmap='gist_gray',vmin=25,vmax=60)\n",
    "\n",
    "ax[1,0].imshow(pred[0,...,0],cmap='gist_gray')\n",
    "ax[1,1].imshow(img[1][0,...,0],cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(fit_generator.flow(mode='validation'))\n",
    "fig, ax = plt.subplots(2,1, figsize=(30,30))\n",
    "ax[0].imshow(img[0][0,...,0],cmap='gist_gray',vmin=10,vmax=60)\n",
    "ax[1].imshow(img[1][0,...,0],cmap='Oranges',alpha=1)\n",
    "ax[1].imshow(safe_log(img[0][0,...,0]),cmap='Blues_r',alpha=0.8, vmin=12, vmax=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.rot90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(img[1][0,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[1][0,...] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[0][0,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[1][0,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[1][0,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = Input(shape=(900,900,4))\n",
    "first_layer = Conv2D(3,(3,3))(new_input)\n",
    "other_layers= model.get_layer(\"block1_conv1\")(first_layer)\n",
    "\n",
    "#Model(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers.pop(0)\n",
    "model.layers\n",
    "\n",
    "newInput = Input(batch_shape=(4,900,900,1))\n",
    "newOutputs = model\n",
    "newModel = Model(newInput, newOutputs)\n",
    "\n",
    "newModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open('/Users/a955nd/Programming/axa_climate/managing_provider/data-provider-docs/fastcat_validate_da/tests/data/integration/test_gsi_jackson_26022020_with_satellite_images/raw_bucket/92e5c126-e9d0-423a-95b6-f5284a8e22c8_satellite_raw_iceye_1581871651_0_v0.tiff') as f: \n",
    "    a=f.read()\n",
    "    b=f.meta\n",
    "    with rasterio.open('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
