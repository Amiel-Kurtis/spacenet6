{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[0;34m(path, mode)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/lib/python3.6/distutils/command/__init__.cpython-36m-x86_64-linux-gnu.so'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-65ec17394af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maffine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAffine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrasterstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_zonal_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rasterstats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_zonal_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraster_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzonal_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_point_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrasterstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrasterstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rasterstats/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem_info\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msysinfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/distutils/system_info.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    163\u001b[0m from numpy.distutils.misc_util import (is_sequence, is_string,\n\u001b[1;32m    164\u001b[0m                                        get_shared_lib_extension)\n\u001b[0;32m--> 165\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcmd_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustomized_ccompiler\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_customized_ccompiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/distutils/command/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 ]\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distutils.command'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistutils_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m __all__ = ['build',\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isfile\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[0;34m(path, mode)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TODO: Ajouter descartes\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "from itertools import permutations, combinations, cycle\n",
    "import os \n",
    "from random import sample, shuffle \n",
    "import gc \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import cv2 as cv\n",
    "import sklearn.metrics\n",
    "import re\n",
    "from pathlib import Path\n",
    "import rasterio \n",
    "from rasterio import features\n",
    "import geopandas as gpd\n",
    "from affine import Affine\n",
    "from rasterstats import gen_zonal_stats\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, EarlyStopping, CSVLogger, TensorBoard, ReduceLROnPlateau\n",
    "import tensorflow as tf \n",
    "from efficientnet import tfkeras as efn \n",
    "import segmentation_models as sm\n",
    "from rasterstats import zonal_stats\n",
    "print(tf.__version__)\n",
    "print(sm.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import wkt\n",
    "\n",
    "from typing import List, Tuple\n",
    "from slacker import Slacker\n",
    "slack = Slacker('xoxp-406617419703-407736556887-975525827328-1c7c24b94d95408268b84ada0b16d937')\n",
    "\n",
    "from system import sizeof_fmt, get_resources_usage\n",
    "from helpers import preprocess_to_display, get_sar_imagery_statistics, get_array_from_tiff, get_id_from_filename, get_polygons_in_image\n",
    "\n",
    "TRAIN_COMMON_PATH = Path('train/AOI_11_Rotterdam')\n",
    "TEST_COMMON_PATH = Path('test_public/AOI_11_Rotterdam')\n",
    "TRAIN_SAR_PATH = TRAIN_COMMON_PATH/'SAR-Intensity'\n",
    "TRAIN_GT_PATH = TRAIN_COMMON_PATH/'train_ground_truth'\n",
    "TEST_SAR_PATH = TEST_COMMON_PATH/'SAR-Intensity'\n",
    "\n",
    "FILENAME_PATTERN = {}\n",
    "FILENAME_PATTERN['train'] = re.compile('SN6_Train_AOI_11_Rotterdam_SAR-Intensity_(\\d*_\\d*_tile_\\d*).tif')\n",
    "FILENAME_PATTERN['validation'] = re.compile('SN6_Train_AOI_11_Rotterdam_SAR-Intensity_(\\d*_\\d*_tile_\\d*).tif')\n",
    "FILENAME_PATTERN['test'] = re.compile('SN6_Test_Public_AOI_11_Rotterdam_SAR-Intensity_(\\d*_\\d*_tile_\\d*).tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lth models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add geopandas & rasterstats to the docker\n",
    "#TODO: add usages in a callback for the fit step\n",
    "#TODO: Learn the \"RGB colors\" and then add it to the second fit, with prediction \n",
    "#TODO: Flip data so that everything has the same orientation vis-à-vis the satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buildings = pd.read_csv(TRAIN_COMMON_PATH/'SummaryData/SN6_Train_AOI_11_Rotterdam_Buildings.csv',engine='python')\n",
    "buildings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FRAC = 0.8\n",
    "N_FIT_DATA = 3401\n",
    "N_TRAIN = int(N_FIT_DATA*0.85)\n",
    "N_VALIDATION = N_FIT_DATA-N_TRAIN\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "STEPS_PER_EPOCH = {}\n",
    "STEPS_PER_EPOCH['test'] = len(os.listdir('test_public/AOI_11_Rotterdam/SAR-Intensity'))//BATCH_SIZE+1\n",
    "STEPS_PER_EPOCH['train'] = N_TRAIN//BATCH_SIZE+1\n",
    "STEPS_PER_EPOCH['validation'] = N_VALIDATION//BATCH_SIZE+1\n",
    "\n",
    "N_EPOCHS = 40\n",
    "LOG_DIR = 'logs/'\n",
    "MODELS_DIR = \"models/\"\n",
    "LOAD_MODEL = True\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lt models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_frac=0.8\n",
    "batch_size = 32\n",
    "n_data_train = int(train_val_frac*N_FIT_DATA)\n",
    "n_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_validation =N_FIT_DATA-n_data_train\n",
    "\n",
    "steps_per_epoch_validation =  n_data_validation//batch_size+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_validation*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING_DATASET_SIZE=200840\n",
    "#TRAINING_DATASET_SIZE =200\n",
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "\n",
    "class SpaceNetPipeline:\n",
    "    def __init__(self, shuffle=False, batch_type='multiple_images',\n",
    "                 batch_size=BATCH_SIZE, train_val_frac = TRAIN_FRAC, \n",
    "                 backbone = 'efficientnetb3', verbose=False):\n",
    "        \n",
    "        #assert not (mode=='test' and shuffle==True), 'Error: in test mode, the values should not be shuffled.'\n",
    "\n",
    "        self.batch_size = batch_size \n",
    "        self.image_path = {'fit':TRAIN_SAR_PATH,\n",
    "                           'test':TEST_SAR_PATH}\n",
    "        self.gt_path = TRAIN_GT_PATH\n",
    "        \n",
    "        self.orientations = pd.read_csv('train/AOI_11_Rotterdam/SummaryData/SAR_orientations.txt',sep=' ', header=None)\n",
    "        self.orientations.columns = [\"image_timestamps\", \"orientation\"]\n",
    "        self.verbose = verbose\n",
    "        self.generators = {}\n",
    "        self.data_ids = {}\n",
    "        self.transforms = {}\n",
    "        self.results_polygons = {}\n",
    "        self.raw_polygons = {}\n",
    "        self.raw_predictions = {}\n",
    "        self.final_results = {}\n",
    "        \n",
    "        self.steps_per_epoch = {}\n",
    "        self.n_data = {}\n",
    "        self.n_data['train'] = int(train_val_frac*N_FIT_DATA)\n",
    "        self.n_data['validation'] = N_FIT_DATA - self.n_data['train']\n",
    "        self.n_data['test'] = len(os.listdir('test_public/AOI_11_Rotterdam/SAR-Intensity'))\n",
    "        \n",
    "        if DEBUG:\n",
    "            self.n_data['test'] = 2\n",
    "            self.n_data['train'] = 2\n",
    "            self.n_data['validation'] = 2\n",
    "        \n",
    "        for mode in ['train','validation','test']:\n",
    "            self.steps_per_epoch[mode] = self.n_data[mode]//self.batch_size\n",
    "            if self.n_data[mode]%self.batch_size != 0:\n",
    "                self.steps_per_epoch[mode]+=1\n",
    "        \n",
    "        self.image_files = {}\n",
    "        self.image_files['fit'] = os.listdir(self.image_path['fit'])\n",
    "        self.image_files['test'] = os.listdir(self.image_path['test'])\n",
    "        if shuffle == True: \n",
    "            shuffle(self.image_files['fit'])\n",
    "        self.data_ids[\"train\"] = self.image_files['fit'][:self.n_data['train']]\n",
    "        self.data_ids[\"validation\"] = self.image_files['fit'][self.n_data['train']:]\n",
    "        if batch_type == 'full_image':\n",
    "            self.generators[\"train\"] = cycle((x for x in self.image_files['fit'][:self.n_data['train']]))\n",
    "            self.generators['validation'] = cycle((x for x in self.image_files['fit'][self.n_data['train']:]))\n",
    "        elif batch_type == 'multiple_images':    \n",
    "            self.generators[\"train\"] = cycle((self.image_files['fit'][i:i+batch_size] \n",
    "                                                   for i in range(0, len(self.image_files['fit'][:self.n_data['train']]), batch_size))) \n",
    "            self.generators['validation'] = cycle((self.image_files['fit'][sn_pipeline.n_data['train']+i:sn_pipeline.n_data['train']+i+batch_size] \n",
    "                                                   for i in range(0, len(self.image_files['fit'][self.n_data['train']:]), batch_size))) \n",
    "\n",
    "        self.data_ids[\"test\"] = self.image_files['test']\n",
    "        self.generators[\"test\"] = (self.image_files['test'][i:i+batch_size] for i in range(0, len(self.image_files['fit'][:self.n_data['train']]), batch_size))\n",
    "        self.backbone = backbone\n",
    "    def print_if_verbose(self, *args, status='always'):\n",
    "        if self.verbose and status=='always':\n",
    "            print(*args)\n",
    "        if self.verbose=='debug' and status=='debug':\n",
    "            print(*args)\n",
    "\n",
    "    def normalize(self, batch, normalization_type=None):\n",
    "        if normalization_type is None:\n",
    "            normalized_batch=batch \n",
    "        elif normalization_type=='divide':\n",
    "            normalized_batch=batch/255\n",
    "        return normalized_batch\n",
    "\n",
    "    def get_xy_image(self, im_id, mode='train'):\n",
    "        if mode in ('train','validation'):\n",
    "            source = 'fit'\n",
    "        else:\n",
    "            source = 'test'\n",
    "        x_image, tsm = get_array_from_tiff(self.image_path[source]/im_id)\n",
    "        if mode in ('train','validation'):\n",
    "            y_image, tsm = get_array_from_tiff(self.gt_path/im_id)\n",
    "            y_image = y_image[0]\n",
    "        else:\n",
    "            y_image = None\n",
    "        return x_image, y_image, tsm\n",
    "\n",
    "    def process_x_batch_list(self, x_batch_list):\n",
    "        x_resized = np.ndarray(shape=(len(x_batch_list),HEIGHT,WIDTH,3))\n",
    "        for i in range(len(x_batch_list)):\n",
    "            for j in range(3):\n",
    "                x_resized[i,...,j] = cv.resize(x_batch_list[i][j],dsize=(WIDTH,HEIGHT))\n",
    "        x_batch_normalized = self.normalize(x_resized)\n",
    "        del x_batch_list\n",
    "        del x_resized\n",
    "        return x_batch_normalized\n",
    "    \n",
    "    def process_y_batch_list(self, y_batch_list):\n",
    "        y_resized = np.ndarray(shape=(len(y_batch_list),HEIGHT,WIDTH))\n",
    "        for i in range(len(y_batch_list)):\n",
    "            y_resized[i] = cv.resize(y_batch_list[i],dsize=(WIDTH,HEIGHT))\n",
    "        y_fixed_orientation = y_resized\n",
    "        y_boolean = np.uint8(y_fixed_orientation > 0)\n",
    "        y_expanded = np.expand_dims(y_boolean,axis=-1)\n",
    "        del y_batch_list\n",
    "        del y_boolean\n",
    "        del y_fixed_orientation\n",
    "        del y_resized\n",
    "        return y_expanded\n",
    "        \n",
    "    def flow(self, mode: str =\"train\", with_ground_truth = True, height: int =137,width: int =236):\n",
    "        '''Run the generator '''\n",
    "        c = 0\n",
    "        self.transforms[mode] = []\n",
    "        while True:\n",
    "            image_ids_to_get = next(self.generators[mode])\n",
    "            x_batch_list = []\n",
    "            y_batch_list = []\n",
    "            for im_id in image_ids_to_get:\n",
    "                c+=1\n",
    "                if c % 10 == 0:\n",
    "                    self.print_if_verbose(f\"\\n INFO - Step n°{c} \")\n",
    "                image_orientation = self.orientations.loc[self.orientations[\"image_timestamps\"]==re.match('.*(\\d{14}_\\d{14})',im_id)[1],\"orientation\"]\n",
    "                self.print_if_verbose(\"\\n INFO - image_ids_to_get:\", im_id,\"\\n\",status='always')\n",
    "                self.print_if_verbose(\"\\n INFO - image_orientation:\",image_orientation)\n",
    "\n",
    "                x_image, y_image, tsm = self.get_xy_image(im_id, mode)\n",
    "                self.transforms[mode].append(tsm)\n",
    "                x_batch_list.append(x_image)\n",
    "\n",
    "                if mode in ('train','validation'):\n",
    "                    self.print_if_verbose(\"\\n INFO - current mode \", mode, status=\"debug\")\n",
    "                    y_batch_list.append(y_image)\n",
    "            x_batch_processed = self.process_x_batch_list(x_batch_list)\n",
    "            \n",
    "            if mode in ('train','validation'):\n",
    "                y_batch_processed = self.process_y_batch_list(y_batch_list)\n",
    "                #self.print_if_verbose(f\"\\n INFO - Yielding train data n°{c}/{self.batch_size/self.files_size['train']}\")\n",
    "                yield x_batch_processed, y_batch_processed\n",
    "            else:\n",
    "                c+=1\n",
    "                if c % 10==0:\n",
    "                    print(f\"\\n INFO - Step n°{c} \")\n",
    "                #self.print_if_verbose(f\"\\n INFO - Yielding {mode} data n°{c}/{self.batch_size/self.files_size[mode]}\")\n",
    "                yield x_batch_processed\n",
    "            gc.collect()\n",
    "    \n",
    "    def get_callbacks(self):\n",
    "        checkpoint = ModelCheckpoint(MODELS_DIR+f\"model_weights--{self.backbone}\"+\"-{epoch:02d}-{val_loss:.4f}--{val_iou_score:.4f}.hdf5\", \n",
    "                                     monitor='val_loss', \n",
    "                                     verbose=1, \n",
    "                                     save_best_only=False, \n",
    "                                     mode='min')\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                      min_delta=0,\n",
    "                                      patience=12,\n",
    "                                      verbose=1, mode='auto')\n",
    "        csv_logger = CSVLogger(LOG_DIR+'training.log')\n",
    "\n",
    "        reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                      patience=4, min_lr=0.0001, verbose =1)\n",
    "        \n",
    "        self.callbacks = [checkpoint, early_stopping, csv_logger, reduce_lr_on_plateau]\n",
    "        #self.callbacks = []\n",
    "    \n",
    "    def get_model(self,weights_path=None):\n",
    "        self.model = sm.Unet(self.backbone, weights=weights_path)\n",
    "        self.model.compile(\n",
    "            'Adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[sm.metrics.iou_score],\n",
    "        )\n",
    "            \n",
    "    def fit(self):\n",
    "        if self.model is None: \n",
    "            raise ValueError(\"Model is not defined yet.\")\n",
    "        self.print_if_verbose(\"\\n INFO - Training...\")\n",
    "        self.history = self.model.fit_generator(\n",
    "           self.flow(mode=\"train\"),\n",
    "           epochs=N_EPOCHS,\n",
    "           steps_per_epoch=self.steps_per_epoch['train'],\n",
    "           validation_steps=self.steps_per_epoch['validation'],\n",
    "           validation_data=self.flow(mode='validation'),\n",
    "           callbacks = self.callbacks\n",
    "    )\n",
    "        \n",
    "    def get_polygons_from_predictions(self, mode, threshold = 0.5):\n",
    "        self.raw_polygons[mode] = []\n",
    "        self.final_results[mode] = []\n",
    "        n_vectorization = len(self.raw_predictions[mode])\n",
    "        t_polygons = tqdm(enumerate(self.raw_predictions[mode][...,0]),\n",
    "                          total=n_vectorization)\n",
    "        self.all_pols = []\n",
    "        aff = Affine(1,0,0,0,-1,900)\n",
    "        for i_image, pred in t_polygons:\n",
    "            t_polygons.set_description(f\"Memory: {get_resources_usage()['memory']} - CPU: {get_resources_usage()['cpu']}\")\n",
    "            if i_image == 0:\n",
    "                plt.imshow(pred)\n",
    "                plt.show()\n",
    "            if i_image==0:\n",
    "                plt.imshow(pred)\n",
    "                plt.show()\n",
    "\n",
    "            pred = cv.resize(pred,(900,900))\n",
    "            boolean_image = np.uint8(pred > threshold)\n",
    "            pols = features.shapes(boolean_image,\n",
    "                                   transform=aff\n",
    "                                  )\n",
    "            pols = [x for x in pols if x[1] ==1]\n",
    "            self.all_pols.append(pols)\n",
    "            \n",
    "            extracted = []\n",
    "            for pol,_ in  pols:\n",
    "                extracted.append(Polygon(pol['coordinates'][0]).wkt)\n",
    "            \n",
    "            extracted_values = zonal_stats(extracted,\n",
    "                                           pred,\n",
    "                                           affine = aff\n",
    "                                          )\n",
    "\n",
    "            extracted_infos = [(x,y['mean']) for x,y in zip(extracted, \n",
    "                                                            extracted_values)]\n",
    "            for pol_wkt,val in extracted_infos:\n",
    "                self.final_results[mode].append((get_id_from_filename(FILENAME_PATTERN[mode],self.data_ids[mode][i_image]), pol_wkt, val))\n",
    "            \n",
    "    def predict(self, mode='train'):\n",
    "        if self.model is None: \n",
    "            raise ValueError(\"Model is not defined yet.\")\n",
    "        with_ground_truth = True\n",
    "        \n",
    "        if mode == 'test':\n",
    "            with_ground_truth=False\n",
    "        print(\"\\n INFO - Predictions...\")\n",
    "        self.raw_predictions[mode] = self.model.predict(\n",
    "            self.flow(mode=mode,with_ground_truth=with_ground_truth),steps=self.steps_per_epoch[mode]\n",
    "        )\n",
    "        print(\"\\n INFO - Flipping predictions...\")\n",
    "        self.raw_predictions[mode] = np.flip(self.raw_predictions[mode], axis=1)\n",
    "        print(\"\\n INFO - Vectorizations...\")\n",
    "        self.get_polygons_from_predictions(mode)\n",
    "        #self.get_confidence()\n",
    "\n",
    "    def format_results(self, mode):\n",
    "        assert self.final_results[mode] is not None\n",
    "        print(\"self.final_results sample:\",self.final_results[mode][:2])\n",
    "        self.final_results[mode] = pd.DataFrame(self.final_results[mode],\n",
    "                                                columns=['ImageId','PolygonWKT_Pix','Confidence']).sort_values(by='ImageId')\n",
    "        \n",
    "    def save_results(self, mode='train'):\n",
    "        self.final_results[mode].to_csv(f'solutions/{mode}_{self.backbone}_{datetime.now().strftime(\"%Y-%m-%dT%H:%M\")}.csv', index=False)\n",
    "        \n",
    "    def run_pipeline(self, fit=True, predict_train = True, predict_validation = True, predict_test = False, weights_path=None):\n",
    "        \n",
    "        self.get_model(weights_path=weights_path)\n",
    "        \n",
    "        if fit:\n",
    "            self.get_callbacks()\n",
    "            self.fit()\n",
    "        \n",
    "        if predict_train:\n",
    "            self.print_if_verbose(\"\\n INFO - Prediction on the train set\")        \n",
    "            self.predict(mode='train')\n",
    "            self.format_results(mode='train')\n",
    "            self.save_results(mode=\"train\")\n",
    "            \n",
    "        if predict_validation:\n",
    "            self.print_if_verbose(\"\\n INFO - Prediction on the validation set\")        \n",
    "            self.predict(mode='validation')\n",
    "            self.format_results(mode='validation')\n",
    "            self.save_results(mode='validation')\n",
    "\n",
    "        if predict_test:\n",
    "            self.print_if_verbose(\"\\n INFO - Prediction on the test set\")        \n",
    "            self.predict(mode='test')\n",
    "            self.format_results(mode='test')\n",
    "            self.save_results(mode='test')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lt models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_pipeline = SpaceNetPipeline(batch_size=32, train_val_frac = TRAIN_FRAC, \n",
    "                               verbose=False, backbone = 'efficientnetb3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    sn_pipeline.run_pipeline(\n",
    "                             fit=True,\n",
    "                             predict_train = True,\n",
    "                             predict_validation = True, \n",
    "                             predict_test = True, \n",
    "                             weights_path='models/model_weights--efficientnetb3-21-0.1158--0.5424.hdf5'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sn_pipeline.data_ids['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=\"test\"\n",
    "sn_pipeline.final_results[mode].to_csv(f'solutions/{mode}_{sn_pipeline.backbone}_{datetime.now().strftime(\"%Y-%m-%dT%H:%M\")}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sn_pipeline.raw_predictions['test'][2,...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_pipeline.data_ids['test'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sn_pipeline.transforms['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.flip(sn_pipeline.raw_predictions['train'],axis=1)[0,...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sn_pipeline.raw_predictions['train'][0,...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_pipeline.raw_predictions['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_pipeline = SpaceNetPipeline(verbose=False, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sn_pipeline.run_pipeline(fit=False, predict_train = True, predict_validation = False, predict_test = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Méthode 1: \n",
    "- Lors de la pred, on sauve les rasters\n",
    "- On vectorise après raster par raster \n",
    "\n",
    "2) Méthode 2: \n",
    "- Pendant le training, on garde les transforme\n",
    "- Après la pred  \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train: \n",
    "java -jar visualizer.jar -image-dir /home/amiel/Projects/competitions/spacenet/spacenet6/train/AOI_11_Rotterdam/SAR-Intensity -truth /home/amiel/Projects/competitions/spacenet/spacenet6/train/AOI_11_Rotterdam/SummaryData/SN6_Train_AOI_11_Rotterdam_Buildings.csv -solution /home/amiel/Projects/competitions/spacenet/spacenet6/solutions/train_2020-04-25T16:31.csv\n",
    "#Test: \n",
    "java -jar visualizer.jar -image-dir /home/amiel/Projects/competitions/spacenet/spacenet6/test_public/AOI_11_Rotterdam/SAR-Intensity -solution /home/amiel/Projects/competitions/spacenet/spacenet6/solutions/test_2020-04-25T22:01.csv\n",
    "\n",
    "        \n",
    "        #Apparemment, pb de définition de l'image (on prédit, mais n'importe quoi )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&²# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
